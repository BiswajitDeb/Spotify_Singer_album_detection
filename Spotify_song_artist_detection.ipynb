{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\r\\nA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\r\\nTouch me gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\r\\nWhy I had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\r\\nA...  \n",
       "1  Take it easy with me, please  \\r\\nTouch me gen...  \n",
       "2  I'll never know why I had to go  \\r\\nWhy I had...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input=pd.read_csv(\"Spotify Million Song Dataset_exported.csv\")\n",
    "Input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=Input.drop(['link'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\r\\nA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>Take it easy with me, please  \\r\\nTouch me gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>I'll never know why I had to go  \\r\\nWhy I had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl   \n",
       "1   ABBA       Andante, Andante   \n",
       "2   ABBA         As Good As New   \n",
       "3   ABBA                   Bang   \n",
       "4   ABBA       Bang-A-Boomerang   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\r\\nA...  \n",
       "1  Take it easy with me, please  \\r\\nTouch me gen...  \n",
       "2  I'll never know why I had to go  \\r\\nWhy I had...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 57650 entries, 0 to 57649\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   artist  57650 non-null  object\n",
      " 1   song    57650 non-null  object\n",
      " 2   text    57650 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist    0\n",
       "song      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I can hear how you work, practising hard  \\r\\nPlaying night and day, wo-oh  \\r\\nAnd it sounds better now, yes you improve  \\r\\nEverytime you play, wo-oh  \\r\\nBut it's bad  \\r\\nYou're so sad  \\r\\nAnd you're only smiling  \\r\\nWhen you play your violin  \\r\\n  \\r\\nDumb dumb diddle, to be your fiddle  \\r\\nTo be so near you and not just hear you  \\r\\nDumb dumb diddle, to be your fiddle  \\r\\nI think then maybe you'd see me, baby  \\r\\nYou'd be mine  \\r\\nAnd we'd be together all the time  \\r\\nWish I was dumb dumb diddle, your darling fiddle  \\r\\n  \\r\\nBut I think, you don't know that I exist  \\r\\nI'm the quiet kind, wo-oh  \\r\\nFrom the day when I first listened to you  \\r\\nYou've been on my mind, wo-oh  \\r\\nYou don't care  \\r\\nIt's not fair  \\r\\nAnd you're only smiling  \\r\\nWhen you play your violin  \\r\\n  \\r\\nDumb dumb diddle, to be your fiddle  \\r\\nTo be so near you and not just hear you  \\r\\nDumb dumb diddle, to be your fiddle  \\r\\nI think then maybe you'd see me, baby  \\r\\nYou'd be mine  \\r\\nAnd we'd be together all the time  \\r\\n  \\r\\nDumb dumb diddle, to be your fiddle  \\r\\nTo be so near you and not just hear you  \\r\\nDumb dumb diddle, to be your fiddle  \\r\\nI think then maybe you'd see me, baby  \\r\\nYou'd be mine  \\r\\nAnd we'd be together all the time  \\r\\nWish I was dumb dumb diddle, your darling find\\r\\n\\r\\n\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample=df[\"text\"][15]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i can hear how you work, practising hard  \\r\\nplaying night and day, wo-oh  \\r\\nand it sounds better now, yes you improve  \\r\\neverytime you play, wo-oh  \\r\\nbut it's bad  \\r\\nyou're so sad  \\r\\nand you're only smiling  \\r\\nwhen you play your violin  \\r\\n  \\r\\ndumb dumb diddle, to be your fiddle  \\r\\nto be so near you and not just hear you  \\r\\ndumb dumb diddle, to be your fiddle  \\r\\ni think then maybe you'd see me, baby  \\r\\nyou'd be mine  \\r\\nand we'd be together all the time  \\r\\nwish i was dumb dumb diddle, your darling fiddle  \\r\\n  \\r\\nbut i think, you don't know that i exist  \\r\\ni'm the quiet kind, wo-oh  \\r\\nfrom the day when i first listened to you  \\r\\nyou've been on my mind, wo-oh  \\r\\nyou don't care  \\r\\nit's not fair  \\r\\nand you're only smiling  \\r\\nwhen you play your violin  \\r\\n  \\r\\ndumb dumb diddle, to be your fiddle  \\r\\nto be so near you and not just hear you  \\r\\ndumb dumb diddle, to be your fiddle  \\r\\ni think then maybe you'd see me, baby  \\r\\nyou'd be mine  \\r\\nand we'd be together all the time  \\r\\n  \\r\\ndumb dumb diddle, to be your fiddle  \\r\\nto be so near you and not just hear you  \\r\\ndumb dumb diddle, to be your fiddle  \\r\\ni think then maybe you'd see me, baby  \\r\\nyou'd be mine  \\r\\nand we'd be together all the time  \\r\\nwish i was dumb dumb diddle, your darling find\\r\\n\\r\\n\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample=df[\"text\"][15]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Indents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "indent=[\"\\n\",\"\\r\",\"\\t\",]\n",
    "\n",
    "def remove_indents(text, indent):\n",
    "    for elements in indent:\n",
    "        text = text.replace(elements, '')\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(lambda elements: remove_indents(elements, indent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i can hear how you work, practising hard  playing night and day, wo-oh  and it sounds better now, yes you improve  everytime you play, wo-oh  but it's bad  you're so sad  and you're only smiling  when you play your violin    dumb dumb diddle, to be your fiddle  to be so near you and not just hear you  dumb dumb diddle, to be your fiddle  i think then maybe you'd see me, baby  you'd be mine  and we'd be together all the time  wish i was dumb dumb diddle, your darling fiddle    but i think, you don't know that i exist  i'm the quiet kind, wo-oh  from the day when i first listened to you  you've been on my mind, wo-oh  you don't care  it's not fair  and you're only smiling  when you play your violin    dumb dumb diddle, to be your fiddle  to be so near you and not just hear you  dumb dumb diddle, to be your fiddle  i think then maybe you'd see me, baby  you'd be mine  and we'd be together all the time    dumb dumb diddle, to be your fiddle  to be so near you and not just hear you  dumb dumb diddle, to be your fiddle  i think then maybe you'd see me, baby  you'd be mine  and we'd be together all the time  wish i was dumb dumb diddle, your darling find\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample=df[\"text\"][15]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing possesive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_words_with_apostrophes(input_string):\n",
    "    # Define a pattern to match words with apostrophes\n",
    "    pattern = r'\\b\\w+\\'\\w+\\b'\n",
    "    \n",
    "    # Use the sub() method to replace all words with apostrophes with an empty string\n",
    "    result = re.sub(pattern, '', input_string)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda elements: remove_words_with_apostrophes(elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i can hear how you work, practising hard  playing night and day, wo-oh  and it sounds better now, yes you improve  everytime you play, wo-oh  but  bad   so sad  and  only smiling  when you play your violin    dumb dumb diddle, to be your fiddle  to be so near you and not just hear you  dumb dumb diddle, to be your fiddle  i think then maybe  see me, baby   be mine  and  be together all the time  wish i was dumb dumb diddle, your darling fiddle    but i think, you  know that i exist   the quiet kind, wo-oh  from the day when i first listened to you   been on my mind, wo-oh  you  care   not fair  and  only smiling  when you play your violin    dumb dumb diddle, to be your fiddle  to be so near you and not just hear you  dumb dumb diddle, to be your fiddle  i think then maybe  see me, baby   be mine  and  be together all the time    dumb dumb diddle, to be your fiddle  to be so near you and not just hear you  dumb dumb diddle, to be your fiddle  i think then maybe  see me, baby   be mine  and  be together all the time  wish i was dumb dumb diddle, your darling find'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample=df[\"text\"][15]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punctuation(input_string):\n",
    "    translation_table = str.maketrans('', '', string.punctuation)\n",
    "    result = input_string.translate(translation_table)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda elements: remove_punctuation(elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i can hear how you work practising hard  playing night and day wooh  and it sounds better now yes you improve  everytime you play wooh  but  bad   so sad  and  only smiling  when you play your violin    dumb dumb diddle to be your fiddle  to be so near you and not just hear you  dumb dumb diddle to be your fiddle  i think then maybe  see me baby   be mine  and  be together all the time  wish i was dumb dumb diddle your darling fiddle    but i think you  know that i exist   the quiet kind wooh  from the day when i first listened to you   been on my mind wooh  you  care   not fair  and  only smiling  when you play your violin    dumb dumb diddle to be your fiddle  to be so near you and not just hear you  dumb dumb diddle to be your fiddle  i think then maybe  see me baby   be mine  and  be together all the time    dumb dumb diddle to be your fiddle  to be so near you and not just hear you  dumb dumb diddle to be your fiddle  i think then maybe  see me baby   be mine  and  be together all the time  wish i was dumb dumb diddle your darling find'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample=df[\"text\"][15]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Biswaji\n",
      "[nltk_data]     Deb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Biswaji\n",
      "[nltk_data]     Deb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words and word not in string.punctuation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hear work practising hard playing night day wooh sounds better yes improve everytime play wooh bad sad smiling play violin dumb dumb diddle fiddle near hear dumb dumb diddle fiddle think maybe see baby mine together time wish dumb dumb diddle darling fiddle think know exist quiet kind wooh day first listened mind wooh care fair smiling play violin dumb dumb diddle fiddle near hear dumb dumb diddle fiddle think maybe see baby mine together time dumb dumb diddle fiddle near hear dumb dumb diddle fiddle think maybe see baby mine together time wish dumb dumb diddle darling find'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample=df[\"text\"][15]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk_tokens = nltk.word_tokenize(sample)\n",
    "# nltk_tokens\n",
    "df['text'] = df['text'].apply(lambda elements: nltk.word_tokenize(elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hear',\n",
       " 'work',\n",
       " 'practising',\n",
       " 'hard',\n",
       " 'playing',\n",
       " 'night',\n",
       " 'day',\n",
       " 'wooh',\n",
       " 'sounds',\n",
       " 'better',\n",
       " 'yes',\n",
       " 'improve',\n",
       " 'everytime',\n",
       " 'play',\n",
       " 'wooh',\n",
       " 'bad',\n",
       " 'sad',\n",
       " 'smiling',\n",
       " 'play',\n",
       " 'violin',\n",
       " 'dumb',\n",
       " 'dumb',\n",
       " 'diddle',\n",
       " 'fiddle',\n",
       " 'near',\n",
       " 'hear',\n",
       " 'dumb',\n",
       " 'dumb',\n",
       " 'diddle',\n",
       " 'fiddle',\n",
       " 'think',\n",
       " 'maybe',\n",
       " 'see',\n",
       " 'baby',\n",
       " 'mine',\n",
       " 'together',\n",
       " 'time',\n",
       " 'wish',\n",
       " 'dumb',\n",
       " 'dumb',\n",
       " 'diddle',\n",
       " 'darling',\n",
       " 'fiddle',\n",
       " 'think',\n",
       " 'know',\n",
       " 'exist',\n",
       " 'quiet',\n",
       " 'kind',\n",
       " 'wooh',\n",
       " 'day',\n",
       " 'first',\n",
       " 'listened',\n",
       " 'mind',\n",
       " 'wooh',\n",
       " 'care',\n",
       " 'fair',\n",
       " 'smiling',\n",
       " 'play',\n",
       " 'violin',\n",
       " 'dumb',\n",
       " 'dumb',\n",
       " 'diddle',\n",
       " 'fiddle',\n",
       " 'near',\n",
       " 'hear',\n",
       " 'dumb',\n",
       " 'dumb',\n",
       " 'diddle',\n",
       " 'fiddle',\n",
       " 'think',\n",
       " 'maybe',\n",
       " 'see',\n",
       " 'baby',\n",
       " 'mine',\n",
       " 'together',\n",
       " 'time',\n",
       " 'dumb',\n",
       " 'dumb',\n",
       " 'diddle',\n",
       " 'fiddle',\n",
       " 'near',\n",
       " 'hear',\n",
       " 'dumb',\n",
       " 'dumb',\n",
       " 'diddle',\n",
       " 'fiddle',\n",
       " 'think',\n",
       " 'maybe',\n",
       " 'see',\n",
       " 'baby',\n",
       " 'mine',\n",
       " 'together',\n",
       " 'time',\n",
       " 'wish',\n",
       " 'dumb',\n",
       " 'dumb',\n",
       " 'diddle',\n",
       " 'darling',\n",
       " 'find']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['look', 'face', 'wonderful', 'face', 'means', 'something', 'special', 'look', 'way', 'smiles', 'sees', 'lucky', 'one', 'fellow', 'kind', 'girl', 'makes', 'feel', 'fine', 'could', 'ever', 'believe', 'could', 'mine', 'kind', 'girl', 'without', 'blue', 'ever', 'leaves', 'could', 'could', 'go', 'walk', 'park', 'holds', 'squeezes', 'hand', 'go', 'walking', 'hours', 'talking', 'things', 'plan', 'kind', 'girl', 'makes', 'feel', 'fine', 'could', 'ever', 'believe', 'could', 'mine', 'kind', 'girl', 'without', 'blue', 'ever', 'leaves', 'could', 'could']\n"
     ]
    }
   ],
   "source": [
    "sample=df['text'][0]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Max length of a word\n",
    "max_len_word=len(df['text'][0][0])\n",
    "for songs in df['text']:\n",
    "    for words in songs:\n",
    "        if len(words)>max_len_word:\n",
    "            max_len_word=len(words)\n",
    "max_len_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "674"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Max number of words in a lyric\n",
    "max_no_words=len(df['text'][0])\n",
    "for songs in df['text']:\n",
    "    if len(songs)>max_no_words:\n",
    "        max_no_words=len(songs)\n",
    "\n",
    "max_no_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total number of unique words in sentence (Vocab size)\n",
    "unique_items_per_list=[]\n",
    "for songs in df['text']:\n",
    "    unique_items_per_list+=set(songs)\n",
    "\n",
    "vocab_size=len(set(unique_items_per_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97978"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "#sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   52    83   840    83   657    82   743    52    18  1393  1170   682\n",
      "     7  3315   245    42   213    28   262    32    57    69    32   109\n",
      "   245    42   116   137    57   707    32    32     8   118  1123  1375\n",
      " 18251   114     8   360   798   401    62   805   245    42   213    28\n",
      "   262    32    57    69    32   109   245    42   116   137    57   707\n",
      "    32    32     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n"
     ]
    }
   ],
   "source": [
    "#Padding\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_no_words, padding='post')\n",
    "print(padded_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = vocab_size + 1  # Adding 1 for the padding token\n",
    "# one_hot_encoded_sequences = to_categorical(padded_sequences, num_classes=vocab_size)\n",
    "# one_hot_encoded_sequences[0]\n",
    "\n",
    "#one_hot_encoded_sequences = to_categorical(padded_sequences, num_classes=vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = padded_sequences\n",
    "Y = df[['artist', 'song']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "lstm_out = 300\n",
    "units=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_no_words))\n",
    "model.add(LSTM(units=units))  # Add more LSTM layers or Dense layers if needed\n",
    "model.add(Dense(units=57650, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 674, 300)          29393400  \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               160400    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 57650)             5822650   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35376450 (134.95 MB)\n",
      "Trainable params: 35376450 (134.95 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46120, 674)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46120, 2)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train_one_hot = to_categorical(Y_train, num_classes=2)\n",
    "# Y_test_one_hot = to_categorical(Y_test,num_classes=2)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert categorical labels to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "Y_train_encoded = label_encoder.fit_transform(Y_train['artist'])\n",
    "Y_test_encoded = label_encoder.transform(Y_test['artist'])\n",
    "\n",
    "# Convert numerical labels to one-hot encoded vectors\n",
    "Y_train_categorical = to_categorical(Y_train_encoded, num_classes=len(label_encoder.classes_))\n",
    "Y_test_categorical = to_categorical(Y_test_encoded, num_classes=len(label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_categorical[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Biswaji Deb\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Biswaji Deb\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Biswaji Deb\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Biswaji Deb\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Biswaji Deb\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\Biswaji Deb\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Biswaji Deb\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Biswaji Deb\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Biswaji Deb\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 2122, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\Biswaji Deb\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py\", line 5560, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 643) and (None, 57650) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m log_history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train_categorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_test_categorical\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\BISWAJ~1\\AppData\\Local\\Temp\\__autograph_generated_file1y6gpmis.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Biswaji Deb\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Biswaji Deb\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Biswaji Deb\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Biswaji Deb\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Biswaji Deb\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\Biswaji Deb\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Biswaji Deb\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Biswaji Deb\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Biswaji Deb\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 2122, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\Biswaji Deb\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py\", line 5560, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 643) and (None, 57650) are incompatible\n"
     ]
    }
   ],
   "source": [
    "log_history = model.fit(X_train, Y_train_categorical, epochs=25, batch_size=64, validation_data=(X_test,Y_test_categorical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('models/emotion_latest.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = len(tokenizer.word_index) + 1  # Adding 1 for the padding token\n",
    "# one_hot_encoded_sequences = to_categorical(padded_sequences, num_classes=vocab_size)\n",
    "\n",
    "# # Step 4: Define the LSTM model\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))\n",
    "# model.add(LSTM(units=units))  # Add more LSTM layers or Dense layers if needed\n",
    "# model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# # Step 5: Compile and train the model\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.fit(one_hot_encoded_sequences, labels, epochs=num_epochs, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
